{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama # will be used for prompting\n",
    "from langchain.vectorstores import Chroma # will be used for vectordb store\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.schema import Document # will be used to store text in vector store \n",
    "import os\n",
    "from kokoro import KPipeline\n",
    "import soundfile as sf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"llama3\")\n",
    "embedding = OllamaEmbeddings(model=\"llama3\")\n",
    "PERSIST_DIR = \"vectordb\"\n",
    "db = Chroma(persist_directory=PERSIST_DIR, embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_recent_unprocessed_idea(db):\n",
    "    results = db.get(include=[\"documents\", \"metadatas\"])\n",
    "\n",
    "    unprocessed = [\n",
    "        (meta[\"timestamp\"], Document(page_content=doc, metadata=meta))\n",
    "        for meta, doc in zip(results[\"metadatas\"], results[\"documents\"])\n",
    "        if not meta.get(\"processed\", False)\n",
    "    ]\n",
    "\n",
    "    if not unprocessed:\n",
    "        print(\"No unprocessed ideas found.\")\n",
    "        return None\n",
    "\n",
    "    unprocessed_sorted = sorted(unprocessed, key=lambda x: x[0], reverse=True)\n",
    "    _, document = unprocessed_sorted[0]\n",
    "\n",
    "    print(f\"Most recent unprocessed idea ID: {document.metadata.get('id')}\")\n",
    "    return document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most recent unprocessed idea ID: ea0acb16-8d50-4b6b-b279-7fb8739695f1\n",
      "Hey everyone, have you ever tried to build an application that can automatically translate text from one language to another? Yeah, it sounds like a challenge! But with Amazon SageMaker Ground Truth, you can make it happen. So what is it? Essentially, Ground Truth lets you label and classify data at scale using human workers or machine learning models. You can think of it like training your own super-powered AI assistant that can help with tasks like sentiment analysis, named entity recognition, and more.\n",
      "\n",
      "Imagine you're building a chatbot for customer support - SageMaker Ground Truth makes it easy to train your bot on large amounts of labeled data, so it can understand what customers are saying and respond accordingly. And did you know that many companies are using Ground Truth to train their AI models for applications like self-driving cars? Yeah, it's that powerful.\n",
      "\n",
      "So how do you set it up? Well, you basically just create a dataset, add some human labeling or use machine learning algorithms to classify the data, and voila! You've got labeled data that can be used to train your own AI models. And here's a cool tip: you can even use Ground Truth to label videos for applications like object detection or facial recognition. Crazy, right?\n",
      "\n",
      "So what do you think? Want to supercharge your machine learning projects with Amazon SageMaker Ground Truth? Let me know in the comments below - what's the most creative way you've used labeled data or human-in-the-loop AI in your projects?\n"
     ]
    }
   ],
   "source": [
    "most_recent_idea = get_most_recent_unprocessed_idea(db)\n",
    "if most_recent_idea:\n",
    "    print(most_recent_idea.page_content) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_text = most_recent_idea.page_content if most_recent_idea else \"Hello from Kokoro TTS!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Defaulting repo_id to hexgrad/Kokoro-82M. Pass repo_id='hexgrad/Kokoro-82M' to suppress this warning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ig-reel-env/lib/python3.11/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/ig-reel-env/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the pipeline\n",
    "pipeline = KPipeline(lang_code='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate audio from the script\n",
    "generator = pipeline(script_text, voice='af_heart', speed=1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all audio chunks and timing information\n",
    "audio_chunks = []\n",
    "subtitle_entries = []\n",
    "current_time = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0: Hey everyone, have you ever tried to build an application that can automatically translate text from one language to another? Yeah, it sounds like a challenge! But with Amazon SageMaker Ground Truth, you can make it happen. So what is it? Essentially, Ground Truth lets you label and classify data at scale using human workers or machine learning models.\n",
      "Chunk 1: You can think of it like training your own super-powered AI assistant that can help with tasks like sentiment analysis, named entity recognition, and more.\n",
      "Chunk 2: Imagine you're building a chatbot for customer support - SageMaker Ground Truth makes it easy to train your bot on large amounts of labeled data, so it can understand what customers are saying and respond accordingly. And did you know that many companies are using Ground Truth to train their AI models for applications like self-driving cars? Yeah, it's that powerful.\n",
      "Chunk 3: So how do you set it up? Well, you basically just create a dataset, add some human labeling or use machine learning algorithms to classify the data, and voila! You've got labeled data that can be used to train your own AI models. And here's a cool tip: you can even use Ground Truth to label videos for applications like object detection or facial recognition. Crazy, right?\n",
      "Chunk 4: So what do you think? Want to supercharge your machine learning projects with Amazon SageMaker Ground Truth? Let me know in the comments below - what's the most creative way you've used labeled data or human-in-the-loop AI in your projects?\n"
     ]
    }
   ],
   "source": [
    "for i, (graphemes, phonemes, audio) in enumerate(generator):\n",
    "    audio_chunks.append(audio)\n",
    "    \n",
    "    # Calculate timing for subtitles (assuming 24000 sample rate)\n",
    "    chunk_duration = len(audio) / 24000  # duration in seconds\n",
    "    start_time = current_time\n",
    "    end_time = current_time + chunk_duration\n",
    "    \n",
    "    # Format times for SRT (HH:MM:SS,mmm)\n",
    "    start_str = f\"{int(start_time//3600):02d}:{int((start_time%3600)//60):02d}:{int(start_time%60):02d},{int((start_time%1)*1000):03d}\"\n",
    "    end_str = f\"{int(end_time//3600):02d}:{int((end_time%3600)//60):02d}:{int(end_time%60):02d},{int((end_time%1)*1000):03d}\"\n",
    "    \n",
    "    subtitle_entries.append({\n",
    "        'index': i + 1,\n",
    "        'start_time': start_str,\n",
    "        'end_time': end_str,\n",
    "        'text': graphemes\n",
    "    })\n",
    "    \n",
    "    current_time = end_time\n",
    "    print(f\"Chunk {i}: {graphemes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated output.wav with 5 chunks\n"
     ]
    }
   ],
   "source": [
    "# Combine all audio chunks into a single file\n",
    "if audio_chunks:\n",
    "    combined_audio = np.concatenate(audio_chunks)\n",
    "    sf.write(\"output.wav\", combined_audio, 24000)\n",
    "    print(f\"Generated output.wav with {len(audio_chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated output.srt with 5 subtitle entries\n"
     ]
    }
   ],
   "source": [
    "def save_ass_from_entries(subtitle_entries, output_path=\"output.ass\"):\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"[Script Info]\\n\")\n",
    "        f.write(\"Title: Centered Subtitles\\n\")\n",
    "        f.write(\"ScriptType: v4.00+\\n\\n\")\n",
    "\n",
    "        f.write(\"[V4+ Styles]\\n\")\n",
    "        f.write(\"Format: Name, Fontname, Fontsize, PrimaryColour, Alignment, MarginL, MarginR, MarginV, Bold\\n\")\n",
    "        f.write(\"Style: Default,Arial,36,&H00FFFFFF,2,10,10,10,1\\n\\n\")  # 2 = center-bottom\n",
    "\n",
    "        f.write(\"[Events]\\n\")\n",
    "        f.write(\"Format: Start, End, Style, Text\\n\")\n",
    "\n",
    "        for entry in subtitle_entries:\n",
    "            start = entry['start_time'].replace(\",\", \".\")  # ASS requires . instead of ,\n",
    "            end = entry['end_time'].replace(\",\", \".\")\n",
    "            text = entry['text'].replace(\"\\n\", \"\\\\N\")  # Line breaks\n",
    "            f.write(f\"Dialogue: {start},{end},Default,{text}\\n\")\n",
    "\n",
    "    print(f\"âœ… Saved styled ASS subtitles to: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ig-reel-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
