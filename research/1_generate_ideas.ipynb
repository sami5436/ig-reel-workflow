{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c32f92f",
   "metadata": {},
   "source": [
    "#### Generate ideas based on a similarity search stored in a vectordb\n",
    "\n",
    "- idea is we grab the vectordb stored locally. if none, exists just create an empty one.\n",
    "- provide that list of ideas i guess? maybe to the ollama prompt and tell it like hey(in a rlly good prompt) that you want to generate a script for instagram reel on a new topic that i ahve not gone over yet from this vectordb store.\n",
    "- prvoide script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c4ab8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama # will be used for prompting\n",
    "from langchain.vectorstores import Chroma # will be used for vectordb store\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.schema import Document # will be used to store text in vector store \n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cdcbf9",
   "metadata": {},
   "source": [
    "Use the same model for embedding text (to compare idea similarity). Instantiate the local Ollama model (LLaMA3 in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "352afafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/90/p8kgtvm572d51cr90k19v_580000gn/T/ipykernel_3404/2493284979.py:1: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"llama3\")\n"
     ]
    }
   ],
   "source": [
    "llm = Ollama(model=\"llama3\")\n",
    "embedding = OllamaEmbeddings(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "620b9240",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSIST_DIR = \"vectordb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d401a98b",
   "metadata": {},
   "source": [
    "Loading vector DB\n",
    "- supposed to be a function that loads the vector_db\n",
    "- if it dne, create one\n",
    "- create an instance of a vector db using chroma with that directory created(or used prev) and the embedding attached too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9304eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/90/p8kgtvm572d51cr90k19v_580000gn/T/ipykernel_3404/1878142402.py:4: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  db = Chroma(persist_directory=PERSIST_DIR, embedding_function=embedding)\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(PERSIST_DIR):\n",
    "    os.makedirs(PERSIST_DIR)\n",
    "\n",
    "db = Chroma(persist_directory=PERSIST_DIR, embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e73f55",
   "metadata": {},
   "source": [
    "This function will return a list of all prev saved ideas from the DB\n",
    "- it'll run a similarity search on the db with the key \"\" --> itll return all documents (ideas)\n",
    "- we will just limit to a 100 for now \n",
    "- also, this should be its own function that returns an array, but for testing purposes, ill just append to an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e66aa5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_ideas = []\n",
    "docs = db.similarity_search(\"\", k=100)\n",
    "for doc in docs:\n",
    "    past_ideas.append(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91996440",
   "metadata": {},
   "source": [
    "Saving a new idea\n",
    "- i am going to create a function for this now thatll be used later in the query function i am about to run \n",
    "- simple idea. create a document object with the idea as the page_content\n",
    "- then add the document to the db object init earlier\n",
    "- dont forget the persist!\n",
    "\n",
    "\n",
    "UPDATE - adding meta data such as a unique id, timestamp (going to be used for getting the most recent), processed (so we can query and not have a large output), and the topic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcf5433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_new_idea(db, idea_text):\n",
    "    # Create metadata\n",
    "    unique_id = str(uuid.uuid4())\n",
    "    timestamp = datetime.utcnow().isoformat()\n",
    "\n",
    "    metadata = {\n",
    "        \"id\": unique_id,\n",
    "        \"timestamp\": timestamp,\n",
    "        \"processed\": False\n",
    "    }\n",
    "\n",
    "    # Create Document object with metadata\n",
    "    doc = Document(page_content=idea_text, metadata=metadata)\n",
    "\n",
    "    # Add to DB and persist\n",
    "    db.add_documents([doc])\n",
    "    db.persist()\n",
    "\n",
    "    print(f\"Idea saved with ID: {unique_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe67a86b",
   "metadata": {},
   "source": [
    "Generate new idea function:\n",
    "- this function will take in past_ideas as a parameter. and it join them as a string with a new line character per idea. if no ideas in the past_ideas array, itll set joined_ideas as None yet.\n",
    "- the prompt will be well strucutred as an f string thatll hold the joined_ideas var in it\n",
    "- the llm will be invoked with teh prompt and expect a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9ac6ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_new_idea(past_ideas):\n",
    "    # Combine all past ideas into a single string, or show \"None yet\" if empty\n",
    "    joined_ideas = \"\\n\".join(past_ideas) if past_ideas else \"None yet.\"\n",
    "\n",
    "    # Craft a well-structured prompt asking for a creative, new idea\n",
    "    prompt = f\"\"\"\n",
    "You are a creative AND technical Instagram assistant. Below is a list of past Instagram video topics:\n",
    "\n",
    "{joined_ideas}\n",
    "\n",
    "Your task is to come up with a completely new and original **60-second Instagram script** idea related to AWS services that hasnâ€™t been covered yet.\n",
    "\n",
    "Make the script informative, engaging, and structured as follows:\n",
    "1. Hook: Catchy opening line to grab attention.\n",
    "2. What it is: Explain the AWS service in plain English.\n",
    "3. When to use it: Mention typical use cases or situations.\n",
    "4. How to use it (high-level): Describe in 1 simple line how a developer might use or set it up.\n",
    "5. Bonus tip or cool fact (optional): Share something surprising or helpful.\n",
    "6. Call to Action: Prompt viewers to engage.\n",
    "\n",
    "Use a casual tone like you're talking to a tech-savvy friend. Avoid repeating old topics from the list above.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    # Send prompt to the LLaMA model and get a generated response\n",
    "    return llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a43432",
   "metadata": {},
   "source": [
    "Since we already kinda init the db (and loaded old prev data too if there even existed in the first place), im not going to include it in the function below. however, ill still include the generate_new_idea function since this is actually calling the llm and will also call the save_new_idea function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35b0384c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_idea_generation(db):\n",
    "    new_idea = generate_new_idea(past_ideas)  # Generate a new idea\n",
    "    save_new_idea(db, new_idea)           # Save the new idea to the DB\n",
    "    return new_idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a82b9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a new Instagram script idea:\n",
      "\n",
      "**Title:** \"Level Up Your Machine Learning Models with AWS SageMaker Autopilot\"\n",
      "\n",
      "**Hook:** \"Are you tired of manually tuning machine learning models? Well, we've got some game-changing news for you!\"\n",
      "\n",
      "**What it is:** \"AWS SageMaker Autopilot is an automated service that helps you optimize your machine learning models without needing to write a single line of code. It's like having a personal data scientist assistant!\"\n",
      "\n",
      "**When to use it:** \"Autopilot is perfect for situations where you're working with large datasets and need to quickly train and deploy accurate models. Think predictive maintenance, customer churn prediction, or natural language processing.\"\n",
      "\n",
      "**How to use it (high-level):** \"Just upload your dataset, select a pre-built model, and let Autopilot do the rest! You can even get real-time feedback on performance metrics to continuously improve your models.\"\n",
      "\n",
      "**Bonus tip or cool fact:** \"Did you know that SageMaker Autopilot uses reinforcement learning to optimize hyperparameters? It's like having a super smart AI coach that helps you make data-driven decisions!\"\n",
      "\n",
      "**Call to Action:** \"Ready to level up your machine learning game? Head over to our website (link in bio) for more information on SageMaker Autopilot and start automating your model development today! Share with a fellow tech enthusiast who's always looking to stay ahead of the curve\"\n",
      "\n",
      "This script is designed to be engaging, informative, and easy to follow, while also highlighting the unique value proposition of AWS SageMaker Autopilot. The tone is casual and conversational, making it perfect for an Instagram video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/90/p8kgtvm572d51cr90k19v_580000gn/T/ipykernel_3404/2143871773.py:4: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  db.persist() # persist changes to disk\n"
     ]
    }
   ],
   "source": [
    "print(run_idea_generation(db))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
