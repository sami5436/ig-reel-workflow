{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c32f92f",
   "metadata": {},
   "source": [
    "#### Generate ideas based on a similarity search stored in a vectordb\n",
    "\n",
    "- idea is we grab the vectordb stored locally. if none, exists just create an empty one.\n",
    "- provide that list of ideas i guess? maybe to the ollama prompt and tell it like hey(in a rlly good prompt) that you want to generate a script for instagram reel on a new topic that i ahve not gone over yet from this vectordb store.\n",
    "- prvoide script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c4ab8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama # will be used for prompting\n",
    "from langchain.vectorstores import Chroma # will be used for vectordb store\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.schema import Document # will be used to store text in vector store \n",
    "import uuid\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cdcbf9",
   "metadata": {},
   "source": [
    "Use the same model for embedding text (to compare idea similarity). Instantiate the local Ollama model (LLaMA3 in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "352afafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/90/p8kgtvm572d51cr90k19v_580000gn/T/ipykernel_57538/2493284979.py:1: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"llama3\")\n"
     ]
    }
   ],
   "source": [
    "llm = Ollama(model=\"llama3\")\n",
    "embedding = OllamaEmbeddings(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "620b9240",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSIST_DIR = \"vectordb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d401a98b",
   "metadata": {},
   "source": [
    "Loading vector DB\n",
    "- supposed to be a function that loads the vector_db\n",
    "- if it dne, create one\n",
    "- create an instance of a vector db using chroma with that directory created(or used prev) and the embedding attached too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9304eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/90/p8kgtvm572d51cr90k19v_580000gn/T/ipykernel_57538/1878142402.py:4: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  db = Chroma(persist_directory=PERSIST_DIR, embedding_function=embedding)\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(PERSIST_DIR):\n",
    "    os.makedirs(PERSIST_DIR)\n",
    "\n",
    "db = Chroma(persist_directory=PERSIST_DIR, embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e73f55",
   "metadata": {},
   "source": [
    "This function will return a list of all prev saved ideas from the DB\n",
    "- it'll run a similarity search on the db with the key \"\" --> itll return all documents (ideas)\n",
    "- we will just limit to a 100 for now \n",
    "- also, this should be its own function that returns an array, but for testing purposes, ill just append to an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e66aa5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_ideas = []\n",
    "docs = db.similarity_search(\"\", k=100)\n",
    "for doc in docs:\n",
    "    past_ideas.append(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91996440",
   "metadata": {},
   "source": [
    "Saving a new idea\n",
    "- i am going to create a function for this now thatll be used later in the query function i am about to run \n",
    "- simple idea. create a document object with the idea as the page_content\n",
    "- then add the document to the db object init earlier\n",
    "- dont forget the persist!\n",
    "\n",
    "\n",
    "UPDATE - adding meta data such as a unique id, timestamp (going to be used for getting the most recent), processed (so we can query and not have a large output)\n",
    "- we just literally do the same thing but have a new parameter for metdata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dcf5433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_new_idea(db, idea_text):\n",
    "    # Create metadata\n",
    "    unique_id = str(uuid.uuid4())\n",
    "    timestamp = datetime.utcnow().isoformat()\n",
    "\n",
    "    metadata = {\n",
    "        \"id\": unique_id,\n",
    "        \"timestamp\": timestamp,\n",
    "        \"processed\": False\n",
    "    }\n",
    "\n",
    "    # Create Document object with metadata\n",
    "    doc = Document(page_content=idea_text, metadata=metadata)\n",
    "\n",
    "    # Add to DB and persist\n",
    "    db.add_documents([doc])\n",
    "    db.persist()\n",
    "\n",
    "    print(f\"Idea saved with ID: {unique_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe67a86b",
   "metadata": {},
   "source": [
    "Generate new idea function:\n",
    "- this function will take in past_ideas as a parameter. and it join them as a string with a new line character per idea. if no ideas in the past_ideas array, itll set joined_ideas as None yet.\n",
    "- the prompt will be well strucutred as an f string thatll hold the joined_ideas var in it\n",
    "- the llm will be invoked with teh prompt and expect a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ac6ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_new_idea(past_ideas):\n",
    "    # Combine all past ideas into a single string, or show \"None yet\" if empty\n",
    "    joined_ideas = \"\\n\".join(past_ideas) if past_ideas else \"None yet.\"\n",
    "\n",
    "    # Craft a well-structured prompt asking for a creative, new idea\n",
    "    prompt = f\"\"\"\n",
    "    You are a creative AND technical Instagram assistant. Below is a list of past Instagram video topics:\n",
    "\n",
    "    {joined_ideas}\n",
    "\n",
    "    Come up with a completely new and original 50-second Instagram video script idea related to AWS services that hasn’t been covered yet.\n",
    "\n",
    "    Your response must include only the raw spoken script — do NOT include a title, do NOT say anything like “Here’s a script,” and do NOT add any labels, bullet points, or section names. Do not explain what you’re doing. Do not add any extra commentary or setup.\n",
    "\n",
    "    Just start the script as if you're speaking to the camera.\n",
    "\n",
    "    The script should:\n",
    "    - Begin with an attention-grabbing hook\n",
    "    - Explain a specific AWS service in casual, plain English\n",
    "    - Mention when or why someone would use it\n",
    "    - Describe in one simple sentence how a developer might set it up or use it\n",
    "    - Optionally include a surprising fact or tip\n",
    "    - End with a call to action to engage the viewer\n",
    "\n",
    "    Speak like you're talking to a tech-savvy friend. Avoid repeating any past topics listed above. Dive right into the script. Nothing else.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Send prompt to the LLaMA model and get a generated response\n",
    "    return llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a43432",
   "metadata": {},
   "source": [
    "Since we already kinda init the db (and loaded old prev data too if there even existed in the first place), im not going to include it in the function below. however, ill still include the generate_new_idea function since this is actually calling the llm and will also call the save_new_idea function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35b0384c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_idea_generation(db):\n",
    "    new_idea = generate_new_idea(past_ideas)  # Generate a new idea\n",
    "    save_new_idea(db, new_idea)           # Save the new idea to the DB\n",
    "    return new_idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a82b9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idea saved with ID: f63e7113-3bca-4248-a484-aea11e882e21\n",
      "Hey everyone, have you ever wondered how e-commerce sites can handle millions of orders and transactions seamlessly? Well, I'm here to tell you that it's all thanks to Amazon Simple Queue Service, or SQS for short.\n",
      "\n",
      "So, what is SQS? Think of it like a message board where different services or microservices can leave notes for each other. When a customer places an order, the website sends a message to SQS, which then forwards it to the relevant teams, like fulfillment and shipping.\n",
      "\n",
      "SQS is especially useful when you have multiple teams working on different parts of the same process. For instance, if your company uses AI-powered chatbots to handle customer inquiries, and another team handles order processing, SQS allows them to communicate with each other without getting bogged down in complex integrations.\n",
      "\n",
      "To set up SQS, developers simply create a queue, define its properties, and then use APIs or SDKs to send and receive messages. And the best part? You can scale your queues as needed, so you're not stuck with limited resources during peak shopping seasons.\n",
      "\n",
      "Here's a fun fact: did you know that SQS is actually used by many of the world's top e-commerce companies, including Amazon itself?\n",
      "\n",
      "So, if you want to learn more about how AWS services like SQS can help your business thrive in today's digital landscape, check out our links in the bio or drop us a comment below. Thanks for watching, and we'll catch you in the next one!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/90/p8kgtvm572d51cr90k19v_580000gn/T/ipykernel_57538/327579714.py:17: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  db.persist()\n"
     ]
    }
   ],
   "source": [
    "print(run_idea_generation(db))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ig-reel-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
