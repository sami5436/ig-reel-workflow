{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c32f92f",
   "metadata": {},
   "source": [
    "#### Generate ideas based on a similarity search stored in a vectordb\n",
    "\n",
    "- idea is we grab the vectordb stored locally. if none, exists just create an empty one.\n",
    "- provide that list of ideas i guess? maybe to the ollama prompt and tell it like hey(in a rlly good prompt) that you want to generate a script for instagram reel on a new topic that i ahve not gone over yet from this vectordb store.\n",
    "- prvoide script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c4ab8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama # will be used for prompting\n",
    "from langchain.vectorstores import Chroma # will be used for vectordb store\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.schema import Document # will be used to store text in vector store \n",
    "import uuid\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cdcbf9",
   "metadata": {},
   "source": [
    "Use the same model for embedding text (to compare idea similarity). Instantiate the local Ollama model (LLaMA3 in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "352afafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/90/p8kgtvm572d51cr90k19v_580000gn/T/ipykernel_21627/2493284979.py:1: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"llama3\")\n"
     ]
    }
   ],
   "source": [
    "llm = Ollama(model=\"llama3\")\n",
    "embedding = OllamaEmbeddings(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "620b9240",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSIST_DIR = \"vectordb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d401a98b",
   "metadata": {},
   "source": [
    "Loading vector DB\n",
    "- supposed to be a function that loads the vector_db\n",
    "- if it dne, create one\n",
    "- create an instance of a vector db using chroma with that directory created(or used prev) and the embedding attached too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9304eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/90/p8kgtvm572d51cr90k19v_580000gn/T/ipykernel_21627/1878142402.py:4: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  db = Chroma(persist_directory=PERSIST_DIR, embedding_function=embedding)\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(PERSIST_DIR):\n",
    "    os.makedirs(PERSIST_DIR)\n",
    "\n",
    "db = Chroma(persist_directory=PERSIST_DIR, embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e73f55",
   "metadata": {},
   "source": [
    "This function will return a list of all prev saved ideas from the DB\n",
    "- it'll run a similarity search on the db with the key \"\" --> itll return all documents (ideas)\n",
    "- we will just limit to a 100 for now \n",
    "- also, this should be its own function that returns an array, but for testing purposes, ill just append to an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e66aa5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_ideas = []\n",
    "docs = db.similarity_search(\"\", k=100)\n",
    "for doc in docs:\n",
    "    past_ideas.append(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91996440",
   "metadata": {},
   "source": [
    "Saving a new idea\n",
    "- i am going to create a function for this now thatll be used later in the query function i am about to run \n",
    "- simple idea. create a document object with the idea as the page_content\n",
    "- then add the document to the db object init earlier\n",
    "- dont forget the persist!\n",
    "\n",
    "\n",
    "UPDATE - adding meta data such as a unique id, timestamp (going to be used for getting the most recent), processed (so we can query and not have a large output)\n",
    "- we just literally do the same thing but have a new parameter for metdata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dcf5433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_new_idea(db, idea_text):\n",
    "    # Create metadata\n",
    "    unique_id = str(uuid.uuid4())\n",
    "    timestamp = datetime.utcnow().isoformat()\n",
    "\n",
    "    metadata = {\n",
    "        \"id\": unique_id,\n",
    "        \"timestamp\": timestamp,\n",
    "        \"processed\": False\n",
    "    }\n",
    "\n",
    "    # Create Document object with metadata\n",
    "    doc = Document(page_content=idea_text, metadata=metadata)\n",
    "\n",
    "    # Add to DB and persist\n",
    "    db.add_documents([doc])\n",
    "    db.persist()\n",
    "\n",
    "    print(f\"Idea saved with ID: {unique_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe67a86b",
   "metadata": {},
   "source": [
    "Generate new idea function:\n",
    "- this function will take in past_ideas as a parameter. and it join them as a string with a new line character per idea. if no ideas in the past_ideas array, itll set joined_ideas as None yet.\n",
    "- the prompt will be well strucutred as an f string thatll hold the joined_ideas var in it\n",
    "- the llm will be invoked with teh prompt and expect a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ac6ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_new_idea(past_ideas):\n",
    "    # Combine all past ideas into a single string, or show \"None yet\" if empty\n",
    "    joined_ideas = \"\\n\".join(past_ideas) if past_ideas else \"None yet.\"\n",
    "\n",
    "    # Craft a well-structured prompt asking for a creative, new idea\n",
    "    prompt = f\"\"\"\n",
    "    You are a creative AND technical Instagram assistant. Below is a list of past Instagram video topics:\n",
    "\n",
    "    {joined_ideas}\n",
    "\n",
    "    Come up with a completely new and original 60-second Instagram video script idea related to AWS services that hasn’t been covered yet.\n",
    "\n",
    "    Your response must include only the raw spoken script — do NOT include a title, do NOT say anything like “Here’s a script,” and do NOT add any labels, bullet points, or section names. Do not explain what you’re doing. Do not add any extra commentary or setup.\n",
    "\n",
    "    Just start the script as if you're speaking to the camera.\n",
    "\n",
    "    The script should:\n",
    "    - Begin with an attention-grabbing hook\n",
    "    - Explain a specific AWS service in casual, plain English\n",
    "    - Mention when or why someone would use it\n",
    "    - Describe in one simple sentence how a developer might set it up or use it\n",
    "    - Optionally include a surprising fact or tip\n",
    "    - End with a call to action to engage the viewer\n",
    "\n",
    "    Speak like you're talking to a tech-savvy friend. Avoid repeating any past topics listed above. Dive right into the script. Nothing else.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Send prompt to the LLaMA model and get a generated response\n",
    "    return llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a43432",
   "metadata": {},
   "source": [
    "Since we already kinda init the db (and loaded old prev data too if there even existed in the first place), im not going to include it in the function below. however, ill still include the generate_new_idea function since this is actually calling the llm and will also call the save_new_idea function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35b0384c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_idea_generation(db):\n",
    "    new_idea = generate_new_idea(past_ideas)  # Generate a new idea\n",
    "    save_new_idea(db, new_idea)           # Save the new idea to the DB\n",
    "    return new_idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a82b9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idea saved with ID: 3bcbdc9f-6bca-4aa1-ba13-9ca2cff18cc3\n",
      "Here's the script:\n",
      "\n",
      "\"Hey there! So, you know how we're always talking about building scalable and secure applications on AWS? Well, today I want to introduce you to a game-changer: Amazon CloudFront. Essentially, it's a content delivery network that helps you quickly distribute your website, application, or even static assets like images or videos to users all over the world.\n",
      "\n",
      "Imagine you're building a popular e-commerce site and you need to serve millions of users with lightning-fast page loads. That's where CloudFront comes in – it caches copies of your static content at edge locations across 25 countries, so when someone requests it, they get served from the nearest location instead of having to travel all the way back to your origin server.\n",
      "\n",
      "For example, if you're building a mobile game and need to deliver high-quality graphics to users worldwide, CloudFront can help reduce latency and improve the overall gaming experience. And the best part? You don't have to worry about setting it up – just create an S3 bucket for your assets, configure CloudFront to distribute them, and voilà! Your app is now globally accessible with blistering speeds.\n",
      "\n",
      "Surprise fact: Did you know that using CloudFront can reduce latency by as much as 50%? That's huge when it comes to improving user engagement and conversion rates. So, if you're building a mission-critical application or e-commerce site, give CloudFront a try – your users will thank you! Thanks for watching, and let me know in the comments below: what's your favorite AWS service?\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/90/p8kgtvm572d51cr90k19v_580000gn/T/ipykernel_21627/327579714.py:17: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  db.persist()\n"
     ]
    }
   ],
   "source": [
    "print(run_idea_generation(db))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ig-reel-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
